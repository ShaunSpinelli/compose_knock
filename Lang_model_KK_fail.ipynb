{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.learner import *\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "import dill as pickle\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/aclImdb/models/TEXT.pkl','rb') as pklf:\n",
    "    TEXT = pickle.load(pklf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tok = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=32; bptt=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34maclImdb\u001b[0m/                    \u001b[01;34mkk_train\u001b[0m/  \u001b[01;34mmodels\u001b[0m/  wiki_en.txt  \u001b[01;34mwiki_valid\u001b[0m/\r\n",
      "cleaned-extra-kk-jokes.txt  \u001b[01;34mkk_valid\u001b[0m/  \u001b[01;34mtmp\u001b[0m/     \u001b[01;34mwiki_train\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "PATH='data/'\n",
    "\n",
    "TRN_PATH = 'kk_train/'\n",
    "VAL_PATH = 'kk_valid/'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'\n",
    "\n",
    "%ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches: 15\n",
      "unique tokens: 37392\n",
      "tokens in training set: 1\n",
      "sentences: 10576\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "f'batches: {len(md.trn_dl)}\\nunique tokens: {md.nt}\\ntokens in training set: {len(md.trn_ds)}\\nsentences: {len(md.trn_ds[0].text)}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " \n",
       " Columns 0 to 10 \n",
       "   3219   3219     61     50      4     61    658  16370     36  19065     61\n",
       "      3      4   1049     45   2080   4282    171    686    137     43  11943\n",
       "   3219     43   1049      0     43      4    337    167     10     61  11943\n",
       "      4     16     43     45     61   4282     45    142      6  19065     43\n",
       "     43     50     61      0   2080     43     38     45    881     27     61\n",
       "     16     45   1049     43    605     61   3219     38     45    144  11943\n",
       "     50      0     68     61      8   4282      3   3219     38   3251     36\n",
       "     45     45      5      0   1232    263   3219      3   3219    182   1594\n",
       "  25970      0   1611    431     68   4071      4   3219      3    144      3\n",
       "     45     43     13    139     34     61     43      4   3219  16223     12\n",
       "  25970     61   1235      2   4624     38     16     43      4      9     35\n",
       "     43      0     45   4026      4   3219     50     16     43      6      8\n",
       "     61  25799     38     45     38      3     45     50     16     93    157\n",
       "  25970     45   3219     38   3219   3219   8487     61     50    339      8\n",
       "   1534     38      3   3219      3      4     45  17014     45     45      2\n",
       " \n",
       " Columns 11 to 21 \n",
       "     61    529   3808      3     16   1801     16     45      0     50     26\n",
       "      0     45    321   3219     50     45     50     38  22604     45    309\n",
       "      0     38     45      4     61   1801     61   3219     43   5116     84\n",
       "      4   3219     38     43   3021     43   3021      3     61     45     10\n",
       "    221      3   3219     16     45     61     45   3219   1247   5116     61\n",
       "      2   3219      3     50   3021   1801   3021      4  22604     43     97\n",
       "   1061      4   3219     61     43      6     43     43     44     61     26\n",
       "     36     43      4   3354     61     62     61     16    567   5116    418\n",
       "     21     16     43      4   3021   1197   3021     50     17     84     10\n",
       "     26     50     16   3354      6     45      6     45    309    363      6\n",
       "     45     45     50     43   1025     38  10837      0     84      3    337\n",
       "     38  10343     45     61    251   3219     62     45     10     12     61\n",
       "   3219      5   4443    461     45      3   1631      0     45   2719    461\n",
       "      3   1377     45     26     38   3219     45     43     38   1467      4\n",
       "   3219     45   4443     32   3219      4     38     61   3219     45     97\n",
       " \n",
       " Columns 22 to 31 \n",
       "   1847     17      3      2     50     45   3063      3     43     43\n",
       "     43     12   3219   4217     61     38     45   3219     13     16\n",
       "     45    120      4     45      0   3219   3063      4      9     50\n",
       "      0    144     43     38      0      3     43     43     45     45\n",
       "     26   6948     16   3219     43   3219     61     16     38  15062\n",
       "     10     32     50      3     61      4   3063     50   3219     45\n",
       "      2     50     61   3219      0     43   1429     45      3  15062\n",
       "  18278     45   1528      4     80     16     12  32685   3219     43\n",
       "      3    941     45     43      8     50   2719     45      4     61\n",
       "     35     38   1528     16   3219     45   1680  32685     43  15062\n",
       "   1187   3219     43     50    101   1247     45     43     16  27296\n",
       "     12      3     61     45    144      5     38     61     50     84\n",
       "     61   3219      0  17708      0      0   3219  32685     61     27\n",
       "     38      4     55     45     82      5      3    302   3679      2\n",
       "   3219     43     26  17708     29   2135   3219     24   3679   3678\n",
       " [torch.cuda.LongTensor of size 15x32 (GPU 0)], Variable containing:\n",
       "      3\n",
       "      4\n",
       "   1049\n",
       "     45\n",
       "   2080\n",
       "   4282\n",
       "    171\n",
       "    686\n",
       "    137\n",
       "     43\n",
       "  11943\n",
       "      0\n",
       "     45\n",
       "    321\n",
       "   3219\n",
       "     50\n",
       "     45\n",
       "     50\n",
       "     38\n",
       "  22604\n",
       "     45\n",
       "    309\n",
       "     43\n",
       "     12\n",
       "   3219\n",
       "   4217\n",
       "     61\n",
       "     38\n",
       "     45\n",
       "   3219\n",
       "     13\n",
       "     16\n",
       "   3219\n",
       "     43\n",
       "   1049\n",
       "      0\n",
       "     43\n",
       "      4\n",
       "    337\n",
       "    167\n",
       "     10\n",
       "     61\n",
       "  11943\n",
       "      0\n",
       "     38\n",
       "     45\n",
       "      4\n",
       "     61\n",
       "   1801\n",
       "     61\n",
       "   3219\n",
       "     43\n",
       "   5116\n",
       "     84\n",
       "     45\n",
       "    120\n",
       "      4\n",
       "     45\n",
       "      0\n",
       "   3219\n",
       "   3063\n",
       "      4\n",
       "      9\n",
       "     50\n",
       "      4\n",
       "     16\n",
       "     43\n",
       "     45\n",
       "     61\n",
       "   4282\n",
       "     45\n",
       "    142\n",
       "      6\n",
       "  19065\n",
       "     43\n",
       "      4\n",
       "   3219\n",
       "     38\n",
       "     43\n",
       "   3021\n",
       "     43\n",
       "   3021\n",
       "      3\n",
       "     61\n",
       "     45\n",
       "     10\n",
       "      0\n",
       "    144\n",
       "     43\n",
       "     38\n",
       "      0\n",
       "      3\n",
       "     43\n",
       "     43\n",
       "     45\n",
       "     45\n",
       "     43\n",
       "     50\n",
       "     61\n",
       "      0\n",
       "   2080\n",
       "     43\n",
       "     38\n",
       "     45\n",
       "    881\n",
       "     27\n",
       "     61\n",
       "    221\n",
       "      3\n",
       "   3219\n",
       "     16\n",
       "     45\n",
       "     61\n",
       "     45\n",
       "   3219\n",
       "   1247\n",
       "   5116\n",
       "     61\n",
       "     26\n",
       "   6948\n",
       "     16\n",
       "   3219\n",
       "     43\n",
       "   3219\n",
       "     61\n",
       "     16\n",
       "     38\n",
       "  15062\n",
       "     16\n",
       "     45\n",
       "   1049\n",
       "     43\n",
       "    605\n",
       "     61\n",
       "   3219\n",
       "     38\n",
       "     45\n",
       "    144\n",
       "  11943\n",
       "      2\n",
       "   3219\n",
       "      3\n",
       "     50\n",
       "   3021\n",
       "   1801\n",
       "   3021\n",
       "      4\n",
       "  22604\n",
       "     43\n",
       "     97\n",
       "     10\n",
       "     32\n",
       "     50\n",
       "      3\n",
       "     61\n",
       "      4\n",
       "   3063\n",
       "     50\n",
       "   3219\n",
       "     45\n",
       "     50\n",
       "      0\n",
       "     68\n",
       "     61\n",
       "      8\n",
       "   4282\n",
       "      3\n",
       "   3219\n",
       "     38\n",
       "   3251\n",
       "     36\n",
       "   1061\n",
       "      4\n",
       "   3219\n",
       "     61\n",
       "     43\n",
       "      6\n",
       "     43\n",
       "     43\n",
       "     44\n",
       "     61\n",
       "     26\n",
       "      2\n",
       "     50\n",
       "     61\n",
       "   3219\n",
       "      0\n",
       "     43\n",
       "   1429\n",
       "     45\n",
       "      3\n",
       "  15062\n",
       "     45\n",
       "     45\n",
       "      5\n",
       "      0\n",
       "   1232\n",
       "    263\n",
       "   3219\n",
       "      3\n",
       "   3219\n",
       "    182\n",
       "   1594\n",
       "     36\n",
       "     43\n",
       "      4\n",
       "   3354\n",
       "     61\n",
       "     62\n",
       "     61\n",
       "     16\n",
       "    567\n",
       "   5116\n",
       "    418\n",
       "  18278\n",
       "     45\n",
       "   1528\n",
       "      4\n",
       "     80\n",
       "     16\n",
       "     12\n",
       "  32685\n",
       "   3219\n",
       "     43\n",
       "  25970\n",
       "      0\n",
       "   1611\n",
       "    431\n",
       "     68\n",
       "   4071\n",
       "      4\n",
       "   3219\n",
       "      3\n",
       "    144\n",
       "      3\n",
       "     21\n",
       "     16\n",
       "     43\n",
       "      4\n",
       "   3021\n",
       "   1197\n",
       "   3021\n",
       "     50\n",
       "     17\n",
       "     84\n",
       "     10\n",
       "      3\n",
       "    941\n",
       "     45\n",
       "     43\n",
       "      8\n",
       "     50\n",
       "   2719\n",
       "     45\n",
       "      4\n",
       "     61\n",
       "     45\n",
       "     43\n",
       "     13\n",
       "    139\n",
       "     34\n",
       "     61\n",
       "     43\n",
       "      4\n",
       "   3219\n",
       "  16223\n",
       "     12\n",
       "     26\n",
       "     50\n",
       "     16\n",
       "   3354\n",
       "      6\n",
       "     45\n",
       "      6\n",
       "     45\n",
       "    309\n",
       "    363\n",
       "      6\n",
       "     35\n",
       "     38\n",
       "   1528\n",
       "     16\n",
       "   3219\n",
       "     45\n",
       "   1680\n",
       "  32685\n",
       "     43\n",
       "  15062\n",
       "  25970\n",
       "     61\n",
       "   1235\n",
       "      2\n",
       "   4624\n",
       "     38\n",
       "     16\n",
       "     43\n",
       "      4\n",
       "      9\n",
       "     35\n",
       "     45\n",
       "     45\n",
       "     50\n",
       "     43\n",
       "   1025\n",
       "     38\n",
       "  10837\n",
       "      0\n",
       "     84\n",
       "      3\n",
       "    337\n",
       "   1187\n",
       "   3219\n",
       "     43\n",
       "     50\n",
       "    101\n",
       "   1247\n",
       "     45\n",
       "     43\n",
       "     16\n",
       "  27296\n",
       "     43\n",
       "      0\n",
       "     45\n",
       "   4026\n",
       "      4\n",
       "   3219\n",
       "     50\n",
       "     16\n",
       "     43\n",
       "      6\n",
       "      8\n",
       "     38\n",
       "  10343\n",
       "     45\n",
       "     61\n",
       "    251\n",
       "   3219\n",
       "     62\n",
       "     45\n",
       "     10\n",
       "     12\n",
       "     61\n",
       "     12\n",
       "      3\n",
       "     61\n",
       "     45\n",
       "    144\n",
       "      5\n",
       "     38\n",
       "     61\n",
       "     50\n",
       "     84\n",
       "     61\n",
       "  25799\n",
       "     38\n",
       "     45\n",
       "     38\n",
       "      3\n",
       "     45\n",
       "     50\n",
       "     16\n",
       "     93\n",
       "    157\n",
       "   3219\n",
       "      5\n",
       "   4443\n",
       "    461\n",
       "     45\n",
       "      3\n",
       "   1631\n",
       "      0\n",
       "     45\n",
       "   2719\n",
       "    461\n",
       "     61\n",
       "   3219\n",
       "      0\n",
       "  17708\n",
       "      0\n",
       "      0\n",
       "   3219\n",
       "  32685\n",
       "     61\n",
       "     27\n",
       "  25970\n",
       "     45\n",
       "   3219\n",
       "     38\n",
       "   3219\n",
       "   3219\n",
       "   8487\n",
       "     61\n",
       "     50\n",
       "    339\n",
       "      8\n",
       "      3\n",
       "   1377\n",
       "     45\n",
       "     26\n",
       "     38\n",
       "   3219\n",
       "     45\n",
       "     43\n",
       "     38\n",
       "   1467\n",
       "      4\n",
       "     38\n",
       "      4\n",
       "     55\n",
       "     45\n",
       "     82\n",
       "      5\n",
       "      3\n",
       "    302\n",
       "   3679\n",
       "      2\n",
       "   1534\n",
       "     38\n",
       "      3\n",
       "   3219\n",
       "      3\n",
       "      4\n",
       "     45\n",
       "  17014\n",
       "     45\n",
       "     45\n",
       "      2\n",
       "   3219\n",
       "     45\n",
       "   4443\n",
       "     32\n",
       "   3219\n",
       "      4\n",
       "     38\n",
       "     61\n",
       "   3219\n",
       "     45\n",
       "     97\n",
       "   3219\n",
       "     43\n",
       "     26\n",
       "  17708\n",
       "     29\n",
       "   2135\n",
       "   3219\n",
       "     24\n",
       "   3679\n",
       "   3678\n",
       "      9\n",
       "   3219\n",
       "   3219\n",
       "      3\n",
       "   3219\n",
       "     43\n",
       "   8487\n",
       "  17014\n",
       "      0\n",
       "     38\n",
       "   4123\n",
       "      4\n",
       "  10343\n",
       "     43\n",
       "     45\n",
       "      3\n",
       "     43\n",
       "   3219\n",
       "      0\n",
       "      3\n",
       "     38\n",
       "     26\n",
       "      3\n",
       "     16\n",
       "   2461\n",
       "     43\n",
       "    181\n",
       "     43\n",
       "      4\n",
       "     12\n",
       "     43\n",
       "     45\n",
       " [torch.cuda.LongTensor of size 480 (GPU 0)])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knock', ',', 'knock', '.', 'who', \"'s\", 'there', '!', 'buggy', '!']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.trn_ds[0].text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "  3219\n",
       "     3\n",
       "  3219\n",
       "     4\n",
       "    43\n",
       "    16\n",
       "    50\n",
       "    45\n",
       " 25970\n",
       "    45\n",
       "[torch.cuda.LongTensor of size 10x1 (GPU 0)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.numericalize([md.trn_ds[0].text[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz = 200  # size of each embedding vector\n",
    "n_layers_act = 500     # number of hidden activations per layer\n",
    "n_layers = 3      # number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = md.get_model(opt_fn, em_sz, n_layers_act, n_layers,\n",
    "               dropouti=0.05, dropout=0.05, wdrop=0.1, dropoute=0.02, dropouth=0.05)\n",
    "learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learner.clip=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load_encoder('adam3_20_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a2e5bb682b41e0855ef7d25f7f15ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      5.237338   5.002712  \n",
      "    1      4.588222   4.373033                            \n",
      "    2      4.133578   4.169879                            \n",
      "    3      3.853429   3.449782                            \n",
      "    4      3.600456   3.76696                             \n",
      "    5      3.393965   3.837112                            \n",
      "    6      3.244129   3.601087                            \n",
      "    7      3.147593   3.158754                            \n",
      "    8      2.984564   3.595135                            \n",
      "    9      2.829187   3.384976                            \n",
      "    10     2.705557   3.213737                            \n",
      "    11     2.605508   3.228901                            \n",
      "    12     2.528559   2.763546                            \n",
      "    13     2.460888   3.321604                            \n",
      "    14     2.420605   3.072621                            \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.0726209]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(3e-4, 4, wds=1e-6, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('kkadam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e98bc8357e4a778f1e27deb9593a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                            \n",
      "    0      2.289193   3.107997  \n",
      "    1      2.11433    2.782552                            \n",
      "    2      1.951139   3.069405                            \n",
      "    3      1.719931   3.197638                            \n",
      "    4      1.551599   3.009426                            \n",
      "    5      1.391812   2.96341                             \n",
      "    6      1.258586   3.023741                            \n",
      "    7      1.155063   2.785676                            \n",
      "    8      1.093497   3.20964                             \n",
      "    9      1.033187   2.846691                            \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8466911]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(3e-3, 1, wds=1e-6, cycle_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('kkadam2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731134b167304feba658204a3458f562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                             \n",
      "    0      0.821904   2.88966   \n",
      "    1      0.811926   2.819601                             \n",
      "    2      0.799529   2.973284                             \n",
      "    3      0.722337   2.981592                             \n",
      "    4      0.73095    2.982337                             \n",
      "    5      0.710055   2.988504                             \n",
      "    6      0.693685   2.991622                             \n",
      "    7      0.635022   3.096914                             \n",
      "    8      0.579769   3.134258                             \n",
      "    9      0.538523   3.03949                              \n",
      "    10     0.498211   2.990397                             \n",
      "    11     0.478751   3.044645                             \n",
      "    12     0.466204   2.851838                             \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8518376]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(3e-3, 3, wds=1e-6, cycle_len=1, cycle_mult=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'peter'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=learner.model\n",
    "ss=\"\"\"peter\"\"\"\n",
    "ss=''.join(w.lower() for w in ss)\n",
    "s = [TEXT.tokenize(ss)]\n",
    "t=TEXT.numericalize(s)\n",
    "' '.join(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size to 1\n",
    "m[0].bs=1\n",
    "# Turn off dropout\n",
    "m.eval()\n",
    "# Reset hidden state\n",
    "m.reset()\n",
    "# Get predictions from model\n",
    "res,*_ = m(t)\n",
    "# Put the batch size back to what it was\n",
    "m[0].bs=bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', ',', '.', '!', 'and', 'the', 'who', '?', '-', \"'s\"]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nexts = torch.topk(res[-1], 10)[1]\n",
    "[TEXT.vocab.itos[o] for o in to_np(nexts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peter \n",
      "\n",
      ", i 'm not talking to you . <eos> knock , knock . who 's there ! carrie ! carrie who ? carrie me home , i m tired ! <eos> knock , knock . who 's there ! carrie ! carrie who ? carrie me home , i m ...\n"
     ]
    }
   ],
   "source": [
    "print(ss,\"\\n\")\n",
    "for i in range(50):\n",
    "    n=res[-1].topk(2)[1]\n",
    "    n = n[1] if n.data[0]==0 else n[0]\n",
    "    print(TEXT.vocab.itos[n.data[0]], end=' ')\n",
    "    res,*_ = m(n[0].unsqueeze(0))\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
